# Update for the 3rd week of self reports

## 1.
- This week I mostly accomplished this weeks goals
- I successfully imported my two made files into their own dataframes
- I was able to join them based on their like words, and export that dataframe to a new excel sheet.
- I coded my scraper and my other program to remove stop words, which will get it ready for NLTk sentiment analysis.

## 2. 

- It takes a long time to run the programs

- I wasn't intending to remove stop words yet until I saw the merged data and realized some words showed up that really didn't need to.

- I'm considering taking data from other sources, but also realize that because of the timeconstraint it may be better to continue for now and improve on technique later


## 3. 

- I want to integrade sentiment analysis into my webscraping code   -2 days

- I want to look into using other sources for my data other than 4chan    -2 days

- I want to make progress on another part of my project, ie the next program that will graph data or maybe even the website    -3 days

## 4.
- I'm using beautiful soup and python pandas

- NLTK sentiment analysis along with other NLTK tools


# Pictures of stop word code

removed stop words for webscraper
![image](https://user-images.githubusercontent.com/35353616/115752880-89472700-a368-11eb-937d-efcdd21fd159.png)

and the reuter corpus

![image](https://user-images.githubusercontent.com/35353616/115753301-0a062300-a369-11eb-8536-fc6296c5a8a9.png)


- you may have noticed it kept in the letter "u" I could easily remedy this by simply adding u to the list of stop words.

![image](https://user-images.githubusercontent.com/35353616/115753594-5e110780-a369-11eb-944c-a8f01ad1c343.png)

Here is a picture of the resulting excel doc from comparing the data, being inspiration to make the data easier to clean. 



