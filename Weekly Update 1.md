# Update for the week of April 5th

## Main updates

- I've decided to shift focus to the rueters corpus. It is a collection of 10,000+ news stories, containing 13.3 million words. 

- My main concerns in using this corpus is it's focus on economic stories. A new corpus might need to be used later, but for now it will suffice. 

- I plan on comparing the data resulting from word frequency to the data taken from my webscraper in order to see the words that are being used the most.

- From here on out, a large part of the project will be seeing what kind of results I get from using different types of data in different ways

- My next step will be to set up a similar file for my webscraper

## Code and results

The following is a test run snaphot of code creates a CSV file from the rueters corpus that contains frequencies of each word that appears in the 10,000 files.

![image](https://user-images.githubusercontent.com/35353616/114290129-36599f80-9a4b-11eb-9248-bf3943a62617.png)

I ran this through the entire 1.3 million words and the result was an excel sheet that contains ~29,000 Unique words with percentages of use. 

[File Rueter_Data.xlsx](https://github.com/Berea-CS-Courses/capstone-project-robinsonkal/files/6291061/File.Rueter_Data.xlsx)

![image](https://user-images.githubusercontent.com/35353616/114290199-bed84000-9a4b-11eb-921a-7169950e18c5.png)


