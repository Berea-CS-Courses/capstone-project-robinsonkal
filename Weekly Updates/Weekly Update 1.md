# Update for the week of April 5th

## 1.

- I've decided to shift focus to the rueters corpus. It is a collection of 10,000+ news stories, containing 13.3 million words. 

- My main concerns in using this corpus is it's focus on economic stories. A new corpus might need to be used later, but for now it will suffice. 

- I plan on comparing the data resulting from word frequency to the data taken from my webscraper in order to see the words that are being used the most.

- This week I created the code that exports a dataframe (made from the reuters corpus) to a csv. In this dataframe was words, word usage, and percentage of use

## 2. 

- It takes a long time to run the program

- A lot of this is new to me so i'm learning on the fly

- Other than the learning curve, it's been going pretty smoothly


## 3. 
- From here on out, a large part of the project will be seeing what kind of results I get from using different types of data in different ways

- My next step will be to set up a similar file for my webscraper

- I want to successfully scrape useful data from the website   -2 days

- I want to clean that data    -1 day

- I want that data put into a new excel doc    -1 day

## 4.
- I'm using beautiful soup and python pandas

- 4chan


## Code and results

The following is a test run snaphot of code creates a CSV file from the rueters corpus that contains frequencies of each word that appears in the 10,000 files.

![image](https://user-images.githubusercontent.com/35353616/114290129-36599f80-9a4b-11eb-9248-bf3943a62617.png)

I ran this through the entire 1.3 million words and the result was an excel sheet that contains ~29,000 Unique words with percentages of use. 

[File Rueter_Data.xlsx](https://github.com/Berea-CS-Courses/capstone-project-robinsonkal/files/6291061/File.Rueter_Data.xlsx)

![image](https://user-images.githubusercontent.com/35353616/114290199-bed84000-9a4b-11eb-921a-7169950e18c5.png)


